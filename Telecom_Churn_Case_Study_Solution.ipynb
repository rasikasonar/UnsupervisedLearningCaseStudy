{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2cca73f",
   "metadata": {},
   "source": [
    "#### Let's import the required libraries for the case study    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd334450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e984b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19498c",
   "metadata": {},
   "source": [
    "###  Data Understanding, Preparation, and Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c26653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have around 172 columns. Let's see their datatypes to know if any are categorical in nature\n",
    "data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d85a51",
   "metadata": {},
   "source": [
    "##### We see that total 9 columns are of object datatype. Let's see the values for these columns and figure out if they would mean any business value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ed8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = list(data.select_dtypes(include=[\"object\"]).columns)\n",
    "object_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[object_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f9dac",
   "metadata": {},
   "source": [
    "###### We see that these columns are of date datatype and they simply describe the last date of the month. \n",
    "###### From a business point of view, last date of the month will not have any impact on the customer's behaviour as it's a static data. Hence we can exclude this column from further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfefb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=object_cols)\n",
    "# Let's see if there are any columns left with object data type\n",
    "data.select_dtypes(include=[\"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9500b",
   "metadata": {},
   "source": [
    "##### As we can see from above, no column is left with object data type. Once we have all the data in numeric data type let's move on to checking if there are any null values present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(verbose=True, show_counts=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_percent = 100*data.isnull().sum()/len(data)\n",
    "missing_data_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ae4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that there are columns having around 74% missing values, we should be removing these columns as they\n",
    "# would not be helpful in model building and making prediction\n",
    "new_vars = missing_data_percent[missing_data_percent.le(40)].index\n",
    "new_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[new_vars]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0605178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have around 136 columns now, let's see if any of them are having higher percentage of missing values\n",
    "(100*data.isnull().sum()/len(data)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7970306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that the missing percentage is very low that is 5 Percent. Lets try imputing zero value for these missing records\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(100*data.isnull().sum()/len(data)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c283d0e8",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2865b62",
   "metadata": {},
   "source": [
    "##### Let's check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_box_plot_in_batches(dataarray):\n",
    "    prev=0\n",
    "    for i in range(15,len(dataarray.columns),16):\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.xticks(rotation=45)\n",
    "        sns.boxplot(data = dataarray.iloc[:,prev:i])\n",
    "        prev = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing with the help of bar plot\n",
    "# Since there are around 136 columns, viewing bar plot for all of the columns on X axis will not be possible.\n",
    "# Let's divide the columns into 15 batches and run the box plot on them\n",
    "\n",
    "show_box_plot_in_batches(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(percentiles=[.25,.5,.75,.90,.95,.99], include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3886ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(array, k=3):\n",
    "    upper_limit = array.mean() + k*array.std()\n",
    "    lower_limit = array.mean() - k*array.std()\n",
    "    array[array<lower_limit] = lower_limit\n",
    "    array[array>upper_limit] = upper_limit\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359558d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.apply(cap_outliers, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a055fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_box_plot_in_batches(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ae86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting X and y dataframe and splitting into train and test dataset\n",
    "data_new = data_new.drop(columns=[\"id\",\"circle_id\"])\n",
    "y = data_new.pop(\"churn_probability\")\n",
    "X = data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde00847",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd1921",
   "metadata": {},
   "source": [
    "##### Let's print correlations for each feature using heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev=0\n",
    "for i in range(15,len(X_train.columns),16):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(pd.DataFrame(X_train, columns=X_train.iloc[:,prev:i].columns).corr())\n",
    "    prev = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ac335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb573994",
   "metadata": {},
   "source": [
    "### Feature Engineering and Variable Transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72475355",
   "metadata": {},
   "source": [
    "##### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ced62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9425bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19982f",
   "metadata": {},
   "source": [
    "##### Applying PCA on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=42)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc820c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282395cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cumu = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19708666",
   "metadata": {},
   "source": [
    "##### Making a scree plot to know how many variables explain the maximum variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8606f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,8])\n",
    "plt.vlines(x=70, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n",
    "plt.hlines(y=0.95, xmax=100, xmin=0, colors=\"g\", linestyles=\"--\")\n",
    "plt.plot(var_cumu)\n",
    "plt.ylabel(\"Cumulative variance explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5ca45",
   "metadata": {},
   "source": [
    "##### As we can see clearly, around 95% of the variance is explained by using 70 features. Hence we will be using 70 variabels to build PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c99420",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc2 = PCA(n_components=70, random_state=42)\n",
    "transformed_data = pc2.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14279f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pca = pd.DataFrame(transformed_data)\n",
    "df_train_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = np.corrcoef(df_train_pca.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the heatmap of the coefficients obtained from pca model. \n",
    "# After the dimentionality reduction, the newly obtain coefficients should not be correlated to each other\n",
    "plt.figure(figsize=[15,15])\n",
    "sns.heatmap(corrmat, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625617c7",
   "metadata": {},
   "source": [
    "##### As we can see, the newly obtained 69 coefficients are not correlated to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bac55b",
   "metadata": {},
   "source": [
    "##### Let's build logistic regression model on top of transformed data received from PCA and predict the churn probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegression = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19665f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodel = logisticRegression.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759717f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying transformation on test data set to reduce dimentionality and get more uncorrelated features\n",
    "df_test_pca = pc2.transform(X_test)\n",
    "df_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8bbcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_train = lrmodel.predict_proba(df_train_pca)\n",
    "y_train_pred = lrmodel.predict(df_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a81371",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_test = lrmodel.predict_proba(df_test_pca)\n",
    "y_test_pred = lrmodel.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:2.2}\".format(metrics.roc_auc_score(y_train, pred_probs_train[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29668a",
   "metadata": {},
   "source": [
    "##### Let's make a confusion matrix to analyze how each class is being predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4337f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbb9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f171c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca07a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3976561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f2032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
